{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for this section\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70/30 for train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random crops of size 227×227 were generated from inside the 256×256 images to feed the first layer of AlexNet. Note that the paper mentions the network inputs to be 224×224, but that is a mistake and the numbers make sense with 227×227 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of data set to use as test\n",
    "test_size = 0.3\n",
    "\n",
    "transform = transforms.Compose([ transforms.CenterCrop(1000), transforms.Resize((227,227)),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "data_set = dset.ImageFolder(root=\"data\",transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(data_set, batch_size=4,shuffle=True,num_workers=2)\n",
    "\n",
    "# obtain training indices that will be used for test\n",
    "num_data = len(data_set)\n",
    "indices = list(range(num_data))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(test_size * num_data))\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler  = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size,\n",
    "                                           sampler = train_sampler, num_workers=num_workers)\n",
    "test_loader  = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                           sampler = test_sampler, num_workers=num_workers)\n",
    "\n",
    "classes = ('ebrus','suminagashis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.alexnet(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if it's available\n",
    "from collections import OrderedDict\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.alexnet(pretrained=True)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(9216, 500)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(500, 2)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "    \n",
    "model.classifier = classifier\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5.. Train loss: 0.055.. Test loss: 0.079.. Test accuracy: 0.983\n",
      "Epoch 1/5.. Train loss: 0.020.. Test loss: 0.081.. Test accuracy: 0.983\n",
      "Epoch 1/5.. Train loss: 0.081.. Test loss: 0.067.. Test accuracy: 0.983\n",
      "Epoch 1/5.. Train loss: 0.042.. Test loss: 0.061.. Test accuracy: 0.983\n",
      "Epoch 1/5.. Train loss: 0.026.. Test loss: 0.069.. Test accuracy: 0.971\n",
      "Epoch 2/5.. Train loss: 0.027.. Test loss: 0.054.. Test accuracy: 0.987\n",
      "Epoch 2/5.. Train loss: 0.007.. Test loss: 0.061.. Test accuracy: 0.987\n",
      "Epoch 2/5.. Train loss: 0.025.. Test loss: 0.063.. Test accuracy: 0.987\n",
      "Epoch 2/5.. Train loss: 0.056.. Test loss: 0.054.. Test accuracy: 0.983\n",
      "Epoch 2/5.. Train loss: 0.005.. Test loss: 0.057.. Test accuracy: 0.983\n",
      "Epoch 2/5.. Train loss: 0.014.. Test loss: 0.053.. Test accuracy: 0.992\n",
      "Epoch 3/5.. Train loss: 0.056.. Test loss: 0.052.. Test accuracy: 0.992\n",
      "Epoch 3/5.. Train loss: 0.010.. Test loss: 0.040.. Test accuracy: 0.988\n",
      "Epoch 3/5.. Train loss: 0.011.. Test loss: 0.030.. Test accuracy: 0.996\n",
      "Epoch 3/5.. Train loss: 0.004.. Test loss: 0.035.. Test accuracy: 0.996\n",
      "Epoch 3/5.. Train loss: 0.006.. Test loss: 0.051.. Test accuracy: 0.979\n",
      "Epoch 4/5.. Train loss: 0.004.. Test loss: 0.059.. Test accuracy: 0.979\n",
      "Epoch 4/5.. Train loss: 0.004.. Test loss: 0.039.. Test accuracy: 0.996\n",
      "Epoch 4/5.. Train loss: 0.011.. Test loss: 0.030.. Test accuracy: 0.992\n",
      "Epoch 4/5.. Train loss: 0.008.. Test loss: 0.033.. Test accuracy: 0.996\n",
      "Epoch 4/5.. Train loss: 0.003.. Test loss: 0.049.. Test accuracy: 0.983\n",
      "Epoch 4/5.. Train loss: 0.001.. Test loss: 0.059.. Test accuracy: 0.983\n",
      "Epoch 5/5.. Train loss: 0.003.. Test loss: 0.052.. Test accuracy: 0.983\n",
      "Epoch 5/5.. Train loss: 0.007.. Test loss: 0.035.. Test accuracy: 0.996\n",
      "Epoch 5/5.. Train loss: 0.002.. Test loss: 0.027.. Test accuracy: 0.996\n",
      "Epoch 5/5.. Train loss: 0.006.. Test loss: 0.028.. Test accuracy: 0.996\n",
      "Epoch 5/5.. Train loss: 0.001.. Test loss: 0.028.. Test accuracy: 0.996\n",
      "Epoch 5/5.. Train loss: 0.001.. Test loss: 0.030.. Test accuracy: 0.996\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 5\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        steps += 1\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    \n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(test_loader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(test_loader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'AlexNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
