{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Classification Neural Network\n",
    "\n",
    "Before using any other famous Deep Neural Networks such as Alexnet or Resnet. We experimented to build our own shallow neural network to see how minimum Convolutional Neural Network would perform classifying Ebru-Suminagashi Images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Selections\n",
    "In order to use state of the art toolset for this research paper we installed CUDA 10 , Pytorch 1.0, Python 3.7 and openCV 3.4 on ubuntu 16.04 with NVDIA 1080 GPU.  https://arxiv.org/pdf/1606.02228.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for this section\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of data set to use as test\n",
    "validation_size = 0.5\n",
    "test_validation_size = 0.4\n",
    "\n",
    "transform = transforms.Compose([ transforms.Resize((256,256)), transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "data_set = dset.ImageFolder(root=\"data\",transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(data_set, batch_size=4,shuffle=True,num_workers=2)\n",
    "\n",
    "# obtain training indices that will be used for test\n",
    "num_data = len(data_set)\n",
    "indices = list(range(num_data))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(test_validation_size * num_data))\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "num_train_data = len(test_idx)\n",
    "split_validation = int(np.floor(validation_size * num_train_data))\n",
    "test_idx, validation_idx = test_idx[split_validation:], test_idx[:split_validation]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler  = SubsetRandomSampler(validation_idx)\n",
    "test_sampler  = SubsetRandomSampler(test_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size,\n",
    "                                           sampler = train_sampler, num_workers=num_workers)\n",
    "validation_loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                           sampler = test_sampler, num_workers=num_workers)\n",
    "test_loader  = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                           sampler = test_sampler, num_workers=num_workers)\n",
    "\n",
    "classes = ('ebrus','suminagashis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrew Ng says previos era (the era before big data) traditional train/test/validation split was 60/20/20. If we still have small dataset traditional ratios still ok to use like our ebru dataset which we have 800 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data -  799\n",
      "Training Data approximately %60 of the total -  480\n",
      "Validation Data approximately %20 of the total -  159\n",
      "Test Data approximately %20 of the total -  160\n"
     ]
    }
   ],
   "source": [
    "print('Total number of data - ', num_data)\n",
    "print('Training Data approximately %60 of the total - ', len(train_idx))\n",
    "print('Validation Data approximately %20 of the total - ', len(validation_idx))\n",
    "print('Test Data approximately %20 of the total - ', len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 3, 256, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(torch.nn.Module):\n",
    "    \n",
    "    #Our batch shape for input x is (3, 32, 32)\n",
    "    #Our batch shape for input x is (3, 256, 256)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        #Input channels = 3, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(18 * 128 * 128, 64)\n",
    "        \n",
    "        #64 input features, 2 output features for our 2 defined classes\n",
    "        self.fc2 = torch.nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (3, 256, 256) to (18, 256, 256)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        #Size changes from (18, 256, 256) to (18, 128, 128)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (18, 128, 128) to (1, 294912)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        x = x.view(-1, 18 * 128 *128)\n",
    "        \n",
    "        #Computes the activation of the first fully connected layer\n",
    "        #Size changes from (1, 294912) to (1, 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #Computes the second fully connected layer (activation applied later)\n",
    "        #Size changes from (1, 64) to (1, 10)\n",
    "        x = self.fc2(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputSize(in_size, kernel_size, stride, padding):\n",
    "\n",
    "    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate=0.001):\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.data\n",
    "            total_train_loss += loss_size.data\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in validation_loader:\n",
    "            \n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(validation_loader)))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 32\n",
      "epochs= 5\n",
      "learning_rate= 0.001\n",
      "==============================\n",
      "Epoch 1, 12% \t train_loss: 17.43 took: 17.90s\n",
      "Epoch 1, 25% \t train_loss: 3.89 took: 17.64s\n",
      "Epoch 1, 37% \t train_loss: 0.99 took: 17.86s\n",
      "Epoch 1, 50% \t train_loss: 0.73 took: 17.73s\n",
      "Epoch 1, 62% \t train_loss: 0.71 took: 17.94s\n",
      "Epoch 1, 75% \t train_loss: 0.51 took: 18.03s\n",
      "Epoch 1, 87% \t train_loss: 0.40 took: 17.67s\n",
      "Epoch 1, 100% \t train_loss: 0.31 took: 17.62s\n",
      "Validation loss = 0.24\n",
      "Epoch 2, 12% \t train_loss: 0.30 took: 17.76s\n",
      "Epoch 2, 25% \t train_loss: 0.27 took: 17.52s\n",
      "Epoch 2, 37% \t train_loss: 0.38 took: 17.52s\n",
      "Epoch 2, 50% \t train_loss: 0.25 took: 17.55s\n",
      "Epoch 2, 62% \t train_loss: 0.25 took: 17.35s\n",
      "Epoch 2, 75% \t train_loss: 0.22 took: 17.32s\n",
      "Epoch 2, 87% \t train_loss: 0.22 took: 17.53s\n",
      "Epoch 2, 100% \t train_loss: 0.20 took: 17.71s\n",
      "Validation loss = 0.13\n",
      "Epoch 3, 12% \t train_loss: 0.24 took: 17.59s\n",
      "Epoch 3, 25% \t train_loss: 0.23 took: 17.55s\n",
      "Epoch 3, 37% \t train_loss: 0.17 took: 17.43s\n",
      "Epoch 3, 50% \t train_loss: 0.21 took: 17.82s\n",
      "Epoch 3, 62% \t train_loss: 0.26 took: 17.63s\n",
      "Epoch 3, 75% \t train_loss: 0.21 took: 17.31s\n",
      "Epoch 3, 87% \t train_loss: 0.10 took: 17.45s\n",
      "Epoch 3, 100% \t train_loss: 0.12 took: 17.94s\n",
      "Validation loss = 0.07\n",
      "Epoch 4, 12% \t train_loss: 0.04 took: 17.20s\n",
      "Epoch 4, 25% \t train_loss: 0.25 took: 17.21s\n",
      "Epoch 4, 37% \t train_loss: 0.12 took: 17.43s\n",
      "Epoch 4, 50% \t train_loss: 0.23 took: 17.87s\n",
      "Epoch 4, 62% \t train_loss: 0.08 took: 18.09s\n",
      "Epoch 4, 75% \t train_loss: 0.11 took: 17.54s\n",
      "Epoch 4, 87% \t train_loss: 0.08 took: 17.53s\n",
      "Epoch 4, 100% \t train_loss: 0.01 took: 17.65s\n",
      "Validation loss = 0.03\n",
      "Epoch 5, 12% \t train_loss: 0.02 took: 17.48s\n",
      "Epoch 5, 25% \t train_loss: 0.04 took: 17.35s\n",
      "Epoch 5, 37% \t train_loss: 0.01 took: 17.26s\n",
      "Epoch 5, 50% \t train_loss: 0.03 took: 17.51s\n",
      "Epoch 5, 62% \t train_loss: 0.02 took: 17.64s\n",
      "Epoch 5, 75% \t train_loss: 0.02 took: 17.29s\n",
      "Epoch 5, 87% \t train_loss: 0.05 took: 17.44s\n",
      "Epoch 5, 100% \t train_loss: 0.02 took: 17.46s\n",
      "Validation loss = 0.04\n",
      "Training finished, took 926.04s\n"
     ]
    }
   ],
   "source": [
    "CNN = SimpleCNN()\n",
    "trainNet(CNN, batch_size=32, n_epochs=5, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
